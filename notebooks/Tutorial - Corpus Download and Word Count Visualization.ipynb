{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern Slavery statements research\n",
    "## Plotting distribution of word counts across statements\n",
    "\n",
    "This notebook will show you how to (1) download the dataset of 12K modern slavery statements, (2) compute the distribution of word counts across statements, and (3) plot that distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install modern-slavery-statements-research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please ensure you can import the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "from modern_slavery_statements_research.download_corpus import download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "access_key_id = 'your access_key_id'\n",
    "secret_access_key = 'your secret_access_key'\n",
    "download(\n",
    "    aws_access_key_id=access_key_id,\n",
    "    aws_secret_access_key=secret_access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](https://github.com/the-future-society/modern-slavery-statements-research/blob/master/README.md#how-to-access-the-data) for more information about data download and how to get these credentials if you do not have them.\n",
    "\n",
    "Note that the download may take several hours, depending on your device and connection.\n",
    "\n",
    "The data will download into one or more directories inside `modern_slavery_statements_research/notebooks`. Each directory is called `scraper_run` followed by a timestamp. Run the following to get the name of the most up-to-date directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scraper_run_dirs = []\n",
    "with os.scandir('./') as iterator:\n",
    "    for entry in iterator:\n",
    "        if entry.path.startswith('./scraper_run'):\n",
    "            scraper_run_dirs.append(entry.path)\n",
    "\n",
    "scraper_run_dir = sorted(scraper_run_dirs)[-1] # get most recent scraper_run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Compute word count distribution\n",
    "Now we have a set of modern slavery statements as `.txt` files in this directory, it's time to count the number of words in each statement. The following code iterates through the statements to create a mapping from word count to the `url_id` of the statements with that word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_counts = []\n",
    "with os.scandir(scraper_run_dir) as iterator:\n",
    "    for txt_file in tqdm(iterator):\n",
    "        assert txt_file.is_file()\n",
    "        assert txt_file.name.endswith(\".txt\")\n",
    "        # extract url_id and word count from the statement\n",
    "        url_id = int(txt_file.name.strip('.txt'))\n",
    "        with open(txt_file.path, encoding=\"utf8\") as f: # some .txt files throw UnicodeDecodeError if this isn't specificied\n",
    "            words_generator = chain.from_iterable(map(str.split, f))\n",
    "            word_count = sum(1 for word in words_generator)\n",
    "        word_counts.append(word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Visualisation\n",
    "Let's plot the word counts as a histogram. We'll use a log scale on the x-axis to improve interpretability, because word counts vary over several orders of magnitude between documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "n, bins, patches = ax.hist(word_counts, bins=np.logspace(0, np.log10(max(word_counts)), 500))\n",
    "ax.set_xlabel('Word counts')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Histogram of word counts of modern slavery statements')\n",
    "ax.set_xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
